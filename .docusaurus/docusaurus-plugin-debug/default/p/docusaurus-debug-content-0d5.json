{"allContent":{"docusaurus-plugin-content-docs":{"default":{"loadedVersions":[{"versionName":"current","label":"Next","banner":null,"badge":false,"noIndex":false,"className":"docs-version-current","path":"/docs","tagsPath":"/docs/tags","editUrl":"https://github.com/Zainscity/AI_-_Humanoid_Robotics_Textbook/tree/main/docs","isLast":true,"routePriority":-1,"sidebarFilePath":"C:\\Files\\code\\Quarter 4\\Backup\\humanoid_robotics_book\\sidebars.js","contentPath":"C:\\Files\\code\\Quarter 4\\Backup\\humanoid_robotics_book\\docs","docs":[{"id":"appendices/hardware-guide","title":"Appendix A: Hardware Guide","description":"This appendix provides a detailed guide to the hardware required to follow the hands-on portions of this book, both for the digital twin workstation and for building a physical robot.","source":"@site/docs/appendices/hardware-guide.md","sourceDirName":"appendices","slug":"/appendices/hardware-guide","permalink":"/docs/appendices/hardware-guide","draft":false,"unlisted":false,"editUrl":"https://github.com/Zainscity/AI_-_Humanoid_Robotics_Textbook/tree/main/docs/appendices/hardware-guide.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Part VI: Capstone Project - The Autonomous Humanoid Assistant","permalink":"/docs/capstone/main"},"next":{"title":"Appendix B: 13-Week Course Roadmap","permalink":"/docs/appendices/weekly-roadmap"}},{"id":"appendices/weekly-roadmap","title":"Appendix B: 13-Week Course Roadmap","description":"This appendix provides a suggested 13-week roadmap for readers following this book as part of a course or structured self-study program. Each week includes reading assignments, practical goals, and a key milestone.","source":"@site/docs/appendices/weekly-roadmap.md","sourceDirName":"appendices","slug":"/appendices/weekly-roadmap","permalink":"/docs/appendices/weekly-roadmap","draft":false,"unlisted":false,"editUrl":"https://github.com/Zainscity/AI_-_Humanoid_Robotics_Textbook/tree/main/docs/appendices/weekly-roadmap.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Appendix A: Hardware Guide","permalink":"/docs/appendices/hardware-guide"}},{"id":"capstone/main","title":"Part VI: Capstone Project - The Autonomous Humanoid Assistant","description":"This capstone project is where we bring together all the concepts, technologies, and skills developed throughout this book. The goal is to build an end-to-end Vision-Language-Action (VLA) pipeline for a humanoid robot, enabling it to act as an autonomous assistant in a simulated home or office environment.","source":"@site/docs/capstone/main.md","sourceDirName":"capstone","slug":"/capstone/main","permalink":"/docs/capstone/main","draft":false,"unlisted":false,"editUrl":"https://github.com/Zainscity/AI_-_Humanoid_Robotics_Textbook/tree/main/docs/capstone/main.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 14: Building Vision-Language-Action (VLA) Pipelines","permalink":"/docs/module4/vla-pipelines"},"next":{"title":"Appendix A: Hardware Guide","permalink":"/docs/appendices/hardware-guide"}},{"id":"foundations/intro","title":"Part I: Foundations of Physical AI & Embodied Intelligence","description":"Welcome to the beginning of your journey into the fascinating world of humanoid robotics. This initial part of the book lays the essential groundwork, introducing the core concepts of Physical AI and Embodied Intelligence. Understanding these principles is crucial before we dive into the practical aspects of building and programming intelligent robots.","source":"@site/docs/foundations/intro.md","sourceDirName":"foundations","slug":"/foundations/intro","permalink":"/docs/foundations/intro","draft":false,"unlisted":false,"editUrl":"https://github.com/Zainscity/AI_-_Humanoid_Robotics_Textbook/tree/main/docs/foundations/intro.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Introduction: Building the Future of Physical AI","permalink":"/docs/"},"next":{"title":"Chapter 1: Understanding ROS 2 Nodes","permalink":"/docs/module1/ros2-nodes"}},{"id":"intro","title":"Introduction: Building the Future of Physical AI","description":"Welcome to \"Physical AI & Humanoid Robotics,\" your comprehensive guide to designing, building, and programming the next generation of intelligent, embodied agents. This book will take you on a journey from the fundamental principles of robotics and AI to the cutting-edge technologies that are making autonomous humanoid robots a reality.","source":"@site/docs/intro.md","sourceDirName":".","slug":"/","permalink":"/docs/","draft":false,"unlisted":false,"editUrl":"https://github.com/Zainscity/AI_-_Humanoid_Robotics_Textbook/tree/main/docs/intro.md","tags":[],"version":"current","frontMatter":{"slug":"/"},"sidebar":"tutorialSidebar","next":{"title":"Part I: Foundations of Physical AI & Embodied Intelligence","permalink":"/docs/foundations/intro"}},{"id":"module1/rclpy","title":"Chapter 5: A Closer Look at rclpy","description":"Throughout this module, we've been using the rclpy library to write our ROS 2 nodes in Python. rclpy (ROS Client Library for Python) is the official Python client library for ROS 2. It provides a high-level, Pythonic interface to the underlying C++ libraries and the ROS 2 middleware, allowing you to build complex robotics applications with ease.","source":"@site/docs/module1/rclpy.md","sourceDirName":"module1","slug":"/module1/rclpy","permalink":"/docs/module1/rclpy","draft":false,"unlisted":false,"editUrl":"https://github.com/Zainscity/AI_-_Humanoid_Robotics_Textbook/tree/main/docs/module1/rclpy.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 4: Handling Long-Running Tasks with ROS 2 Actions","permalink":"/docs/module1/ros2-actions"},"next":{"title":"Chapter 6: Modeling a Humanoid with URDF","permalink":"/docs/module2/urdf-modeling"}},{"id":"module1/ros2-actions","title":"Chapter 4: Handling Long-Running Tasks with ROS 2 Actions","description":"For tasks that take a long time to complete and need to provide feedback along the way, neither topics nor services are a perfect fit. ROS 2 provides another communication pattern called actions for exactly these scenarios. Actions are the backbone of complex robotics behaviors like navigation, manipulation, and executing multi-step sequences.","source":"@site/docs/module1/ros2-actions.md","sourceDirName":"module1","slug":"/module1/ros2-actions","permalink":"/docs/module1/ros2-actions","draft":false,"unlisted":false,"editUrl":"https://github.com/Zainscity/AI_-_Humanoid_Robotics_Textbook/tree/main/docs/module1/ros2-actions.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 3: Using ROS 2 Services for Request/Response","permalink":"/docs/module1/ros2-services"},"next":{"title":"Chapter 5: A Closer Look at rclpy","permalink":"/docs/module1/rclpy"}},{"id":"module1/ros2-nodes","title":"Chapter 1: Understanding ROS 2 Nodes","description":"A ROS 2 system is a distributed network of processes called nodes. Each node is a fundamental unit of computation and should be responsible for a single, modular purpose (e.g., one node for controlling wheel motors, one for processing camera images, and another for planning paths). This chapter explains what nodes are and how to create and manage them.","source":"@site/docs/module1/ros2-nodes.md","sourceDirName":"module1","slug":"/module1/ros2-nodes","permalink":"/docs/module1/ros2-nodes","draft":false,"unlisted":false,"editUrl":"https://github.com/Zainscity/AI_-_Humanoid_Robotics_Textbook/tree/main/docs/module1/ros2-nodes.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Part I: Foundations of Physical AI & Embodied Intelligence","permalink":"/docs/foundations/intro"},"next":{"title":"Chapter 2: Communicating with ROS 2 Topics","permalink":"/docs/module1/ros2-topics"}},{"id":"module1/ros2-services","title":"Chapter 3: Using ROS 2 Services for Request/Response","description":"While topics are great for continuous data streams, sometimes you need a direct request/response interaction between nodes. This is where services come in. This chapter will introduce you to the service communication pattern in ROS 2.","source":"@site/docs/module1/ros2-services.md","sourceDirName":"module1","slug":"/module1/ros2-services","permalink":"/docs/module1/ros2-services","draft":false,"unlisted":false,"editUrl":"https://github.com/Zainscity/AI_-_Humanoid_Robotics_Textbook/tree/main/docs/module1/ros2-services.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 2: Communicating with ROS 2 Topics","permalink":"/docs/module1/ros2-topics"},"next":{"title":"Chapter 4: Handling Long-Running Tasks with ROS 2 Actions","permalink":"/docs/module1/ros2-actions"}},{"id":"module1/ros2-topics","title":"Chapter 2: Communicating with ROS 2 Topics","description":"Nodes communicate with each other by publishing messages to topics. Topics are named buses over which nodes exchange messages. This chapter explains how to use topics to send and receive data.","source":"@site/docs/module1/ros2-topics.md","sourceDirName":"module1","slug":"/module1/ros2-topics","permalink":"/docs/module1/ros2-topics","draft":false,"unlisted":false,"editUrl":"https://github.com/Zainscity/AI_-_Humanoid_Robotics_Textbook/tree/main/docs/module1/ros2-topics.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 1: Understanding ROS 2 Nodes","permalink":"/docs/module1/ros2-nodes"},"next":{"title":"Chapter 3: Using ROS 2 Services for Request/Response","permalink":"/docs/module1/ros2-services"}},{"id":"module2/gazebo-simulation","title":"Chapter 7: Simulating the World with Gazebo","description":"With a URDF model in hand, we can bring our robot to life in a simulated environment. Gazebo is a powerful 3D robotics simulator that allows you to test your robot's design and control algorithms in a realistic virtual world before deploying them on a physical robot.","source":"@site/docs/module2/gazebo-simulation.md","sourceDirName":"module2","slug":"/module2/gazebo-simulation","permalink":"/docs/module2/gazebo-simulation","draft":false,"unlisted":false,"editUrl":"https://github.com/Zainscity/AI_-_Humanoid_Robotics_Textbook/tree/main/docs/module2/gazebo-simulation.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 6: Modeling a Humanoid with URDF","permalink":"/docs/module2/urdf-modeling"},"next":{"title":"Chapter 8: High-Fidelity Simulation with Unity","permalink":"/docs/module2/unity-rendering"}},{"id":"module2/unity-rendering","title":"Chapter 8: High-Fidelity Simulation with Unity","description":"While Gazebo is excellent for physics simulation, the Unity game engine offers a powerful alternative for creating high-fidelity, visually rich simulation environments. Unity is particularly strong in areas like realistic rendering, complex sensor simulation, and creating large, interactive worlds, making it an ideal choice for training and testing perception and HRI algorithms.","source":"@site/docs/module2/unity-rendering.md","sourceDirName":"module2","slug":"/module2/unity-rendering","permalink":"/docs/module2/unity-rendering","draft":false,"unlisted":false,"editUrl":"https://github.com/Zainscity/AI_-_Humanoid_Robotics_Textbook/tree/main/docs/module2/unity-rendering.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 7: Simulating the World with Gazebo","permalink":"/docs/module2/gazebo-simulation"},"next":{"title":"Chapter 9: NVIDIA Isaac Sim for Advanced Simulation","permalink":"/docs/module3/isaac-sim-basics"}},{"id":"module2/urdf-modeling","title":"Chapter 6: Modeling a Humanoid with URDF","description":"Before we can simulate our robot, we need a detailed digital model of it. In ROS, the standard format for describing a robot's structure is the Unified Robot Description Format (URDF). This chapter covers the essentials of creating a URDF file, from basic links and joints to more advanced concepts needed for accurate simulation.","source":"@site/docs/module2/urdf-modeling.md","sourceDirName":"module2","slug":"/module2/urdf-modeling","permalink":"/docs/module2/urdf-modeling","draft":false,"unlisted":false,"editUrl":"https://github.com/Zainscity/AI_-_Humanoid_Robotics_Textbook/tree/main/docs/module2/urdf-modeling.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 5: A Closer Look at rclpy","permalink":"/docs/module1/rclpy"},"next":{"title":"Chapter 7: Simulating the World with Gazebo","permalink":"/docs/module2/gazebo-simulation"}},{"id":"module3/isaac-ros","title":"Chapter 10: Accelerating Robotics with Isaac ROS","description":"NVIDIA Isaac ROS is a collection of hardware-accelerated packages for ROS 2, specifically designed to leverage the power of NVIDIA GPUs and Jetson platforms. It provides high-performance, AI-accelerated capabilities for common robotics tasks, enabling developers to build and deploy complex perception and navigation pipelines more efficiently.","source":"@site/docs/module3/isaac-ros.md","sourceDirName":"module3","slug":"/module3/isaac-ros","permalink":"/docs/module3/isaac-ros","draft":false,"unlisted":false,"editUrl":"https://github.com/Zainscity/AI_-_Humanoid_Robotics_Textbook/tree/main/docs/module3/isaac-ros.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 9: NVIDIA Isaac Sim for Advanced Simulation","permalink":"/docs/module3/isaac-sim-basics"},"next":{"title":"Chapter 11: Bipedal Navigation with Nav2","permalink":"/docs/module3/nav2-biped"}},{"id":"module3/isaac-sim-basics","title":"Chapter 9: NVIDIA Isaac Sim for Advanced Simulation","description":"NVIDIA Isaac Sim is a scalable robotics simulation platform and synthetic data generation tool. Built on the NVIDIA Omniverseâ„¢ platform, it is designed to create physically accurate, photorealistic simulations that are essential for developing, testing, and training AI-based robots in a virtual environment before deploying them to the real world.","source":"@site/docs/module3/isaac-sim-basics.md","sourceDirName":"module3","slug":"/module3/isaac-sim-basics","permalink":"/docs/module3/isaac-sim-basics","draft":false,"unlisted":false,"editUrl":"https://github.com/Zainscity/AI_-_Humanoid_Robotics_Textbook/tree/main/docs/module3/isaac-sim-basics.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 8: High-Fidelity Simulation with Unity","permalink":"/docs/module2/unity-rendering"},"next":{"title":"Chapter 10: Accelerating Robotics with Isaac ROS","permalink":"/docs/module3/isaac-ros"}},{"id":"module3/nav2-biped","title":"Chapter 11: Bipedal Navigation with Nav2","description":"Navigating in complex, human-centric environments is a fundamental capability for any humanoid robot. For wheeled robots, ROS 2's Nav2 stack is the de facto standard, providing a robust and modular solution. However, adapting Nav2 for a bipedal humanoid presents unique challenges that require significant customization of its core components.","source":"@site/docs/module3/nav2-biped.md","sourceDirName":"module3","slug":"/module3/nav2-biped","permalink":"/docs/module3/nav2-biped","draft":false,"unlisted":false,"editUrl":"https://github.com/Zainscity/AI_-_Humanoid_Robotics_Textbook/tree/main/docs/module3/nav2-biped.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 10: Accelerating Robotics with Isaac ROS","permalink":"/docs/module3/isaac-ros"},"next":{"title":"Chapter 12: Voice Control with OpenAI Whisper","permalink":"/docs/module4/whisper"}},{"id":"module4/llm-planning","title":"Chapter 13: Cognitive Planning with Large Language Models","description":"Once a robot can perceive its environment and understand human speech, the next frontier is enabling it to reason about high-level commands and generate a sequence of actions to achieve them. This is where Large Language Models (LLMs) like GPT-4 and Claude 3 serve as a \"cognitive engine\" for the robot, translating natural language into actionable plans.","source":"@site/docs/module4/llm-planning.md","sourceDirName":"module4","slug":"/module4/llm-planning","permalink":"/docs/module4/llm-planning","draft":false,"unlisted":false,"editUrl":"https://github.com/Zainscity/AI_-_Humanoid_Robotics_Textbook/tree/main/docs/module4/llm-planning.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 12: Voice Control with OpenAI Whisper","permalink":"/docs/module4/whisper"},"next":{"title":"Chapter 14: Building Vision-Language-Action (VLA) Pipelines","permalink":"/docs/module4/vla-pipelines"}},{"id":"module4/vla-pipelines","title":"Chapter 14: Building Vision-Language-Action (VLA) Pipelines","description":"We've now explored the key components of an intelligent robotics system: vision (perception), language (speech recognition and understanding), and action (robot control). The final step is to integrate these components into a cohesive Vision-Language-Action (VLA) pipeline, which is the architecture that brings everything together to create a truly interactive and intelligent robot.","source":"@site/docs/module4/vla-pipelines.md","sourceDirName":"module4","slug":"/module4/vla-pipelines","permalink":"/docs/module4/vla-pipelines","draft":false,"unlisted":false,"editUrl":"https://github.com/Zainscity/AI_-_Humanoid_Robotics_Textbook/tree/main/docs/module4/vla-pipelines.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 13: Cognitive Planning with Large Language Models","permalink":"/docs/module4/llm-planning"},"next":{"title":"Part VI: Capstone Project - The Autonomous Humanoid Assistant","permalink":"/docs/capstone/main"}},{"id":"module4/whisper","title":"Chapter 12: Voice Control with OpenAI Whisper","description":"For humanoid robots to interact naturally and intuitively with humans, voice control is a crucial capability. OpenAI's Whisper is a state-of-the-art automatic speech recognition (ASR) model that can accurately transcribe human speech into text, making it an excellent foundation for building voice-controlled robotics systems.","source":"@site/docs/module4/whisper.md","sourceDirName":"module4","slug":"/module4/whisper","permalink":"/docs/module4/whisper","draft":false,"unlisted":false,"editUrl":"https://github.com/Zainscity/AI_-_Humanoid_Robotics_Textbook/tree/main/docs/module4/whisper.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 11: Bipedal Navigation with Nav2","permalink":"/docs/module3/nav2-biped"},"next":{"title":"Chapter 13: Cognitive Planning with Large Language Models","permalink":"/docs/module4/llm-planning"}}],"drafts":[],"sidebars":{"tutorialSidebar":[{"type":"doc","id":"intro"},{"type":"category","label":"Foundations","items":[{"type":"doc","id":"foundations/intro"}],"collapsed":true,"collapsible":true},{"type":"category","label":"Module 1: ROS 2","items":[{"type":"doc","id":"module1/ros2-nodes"},{"type":"doc","id":"module1/ros2-topics"},{"type":"doc","id":"module1/ros2-services"},{"type":"doc","id":"module1/ros2-actions"},{"type":"doc","id":"module1/rclpy"}],"collapsed":true,"collapsible":true},{"type":"category","label":"Module 2: Simulation","items":[{"type":"doc","id":"module2/urdf-modeling"},{"type":"doc","id":"module2/gazebo-simulation"},{"type":"doc","id":"module2/unity-rendering"}],"collapsed":true,"collapsible":true},{"type":"category","label":"Module 3: NVIDIA Isaac","items":[{"type":"doc","id":"module3/isaac-sim-basics"},{"type":"doc","id":"module3/isaac-ros"},{"type":"doc","id":"module3/nav2-biped"}],"collapsed":true,"collapsible":true},{"type":"category","label":"Module 4: VLA","items":[{"type":"doc","id":"module4/whisper"},{"type":"doc","id":"module4/llm-planning"},{"type":"doc","id":"module4/vla-pipelines"}],"collapsed":true,"collapsible":true},{"type":"category","label":"Capstone Project","items":[{"type":"doc","id":"capstone/main"}],"collapsed":true,"collapsible":true},{"type":"category","label":"Appendices","items":[{"type":"doc","id":"appendices/hardware-guide"},{"type":"doc","id":"appendices/weekly-roadmap"}],"collapsed":true,"collapsible":true}]}}]}},"docusaurus-plugin-content-pages":{"default":[{"type":"jsx","permalink":"/","source":"@site/src/pages/index.js"}]},"docusaurus-plugin-debug":{},"docusaurus-plugin-svgr":{},"docusaurus-theme-classic":{},"docusaurus-bootstrap-plugin":{},"docusaurus-mdx-fallback-plugin":{}}}