# Appendix B: 13-Week Course Roadmap

This appendix provides a suggested 13-week roadmap for readers following this book as part of a course or structured self-study program. Each week includes reading assignments, practical goals, and a key milestone.

---

### **Part I: Foundations (Weeks 1-2)**

-   **Week 1: Introduction to Physical AI & Setup**
    -   **Reading**: Part I: Foundations of Physical AI & Embodied Intelligence.
    -   **Goal**: Set up your development workstation (Ubuntu 22.04, ROS 2 Humble, NVIDIA drivers) or cloud environment. Clone the book's source code repository.
    -   **Milestone**: Successfully run `ros2 doctor` with no errors.

-   **Week 2: ROS 2 Core Concepts**
    -   **Reading**: Module 1, Chapters 1-2 (Nodes, Topics).
    -   **Goal**: Write and run your first ROS 2 nodes. Create publishers and subscribers to pass messages between them. Use command-line tools like `ros2 topic echo`.
    -   **Milestone**: Create a simple "talker" and "listener" node package from scratch.

---

### **Part II: Building the Digital Twin (Weeks 3-5)**

-   **Week 3: ROS 2 Services and Actions**
    -   **Reading**: Module 1, Chapters 3-5 (Services, Actions, rclpy).
    -   **Goal**: Implement service clients/servers and action clients/servers. Understand the difference between synchronous and asynchronous communication.
    -   **Milestone**: Create a simple calculator service and a "timer" action.

-   **Week 4: Robot Modeling**
    -   **Reading**: Module 2, Chapter 6 (URDF Modeling).
    -   **Goal**: Learn the fundamentals of URDF and XACRO. Create a URDF for a simple robot arm.
    -   **Milestone**: Visualize your robot model in RViz2 and see it respond to joint state commands.

-   **Week 5: Simulation Environments**
    -   **Reading**: Module 2, Chapters 7-8 (Gazebo, Unity).
    -   **Goal**: Spawn your robot model in both Gazebo and Unity. Learn how to create a basic world file and interact with the simulated robot.
    -   **Milestone**: Drive your simulated robot in Gazebo by publishing velocity commands to a ROS 2 topic.

---

### **Part III: Advanced Simulation & Perception (Weeks 6-8)**

-   **Week 6: Introduction to NVIDIA Isaac Sim**
    -   **Reading**: Module 3, Chapter 9 (Isaac Sim Basics).
    -   **Goal**: Install and explore the Isaac Sim environment. Learn the basics of the Python API for controlling the simulation.
    -   **Milestone**: Re-create your simulation environment from Week 5 inside Isaac Sim.

-   **Week 7: Accelerated Perception with Isaac ROS**
    -   **Reading**: Module 3, Chapter 10 (Isaac ROS).
    -   **Goal**: Set up an Isaac ROS perception pipeline. Use Isaac ROS packages for AprilTag detection or stereo depth estimation.
    -   **Milestone**: Detect an AprilTag in your Isaac Sim environment and see its pose published to a ROS 2 topic.

-   **Week 8: Bipedal Navigation with Nav2**
    -   **Reading**: Module 3, Chapter 11 (Nav2 for Bipeds).
    -   **Goal**: Understand the challenges of adapting Nav2 for humanoids. Set up a basic Nav2 configuration for a simulated wheeled robot as a baseline.
    -   **Milestone**: Send a navigation goal to a simulated TurtleBot in Gazebo and have it successfully navigate to the goal.

---

### **Part IV: AI and VLA Pipelines (Weeks 9-11)**

-   **Week 9: Voice Interaction**
    -   **Reading**: Module 4, Chapter 12 (Whisper).
    -   **Goal**: Set up a ROS 2 node that uses Whisper (or `whisper.cpp`) to transcribe audio from a microphone.
    -   **Milestone**: Speak a command into your microphone and see the transcribed text published to a ROS 2 topic.

-   **Week 10: LLM-based Planning**
    -   **Reading**: Module 4, Chapter 13 (LLM Planning).
    -   **Goal**: Create a ROS 2 node that takes transcribed text, formats it into a prompt for an LLM, and calls the LLM API to generate a plan.
    -   **Milestone**: Give a text command like "pick up the red block" and receive a structured JSON plan from the LLM.

-   **Week 11: Building the VLA Pipeline**
    -   **Reading**: Module 4, Chapter 14 (VLA Pipelines).
    -   **Goal**: Integrate the perception, language, and planning components into a single, cohesive VLA pipeline.
    -   **Milestone**: Give a voice command and see the corresponding plan generated by the LLM, based on what the robot is currently "seeing" in simulation.

---

### **Part V: Capstone Project (Weeks 12-13)**

-   **Week 12: Capstone Project Implementation**
    -   **Reading**: Part VI: Capstone Project.
    -   **Goal**: Begin implementing the full end-to-end system described in the capstone chapter. Focus on integrating the navigation and manipulation controllers.
    -   **Milestone**: The robot can successfully navigate to an object in the simulation based on a language command.

-   **Week 13: Final Integration and Testing**
    -   **Goal**: Complete the capstone project. The robot should be able to perceive, navigate, and manipulate objects based on high-level voice commands.
    -   **Milestone**: Demonstrate the full VLA pipeline with a live demo in Isaac Sim.